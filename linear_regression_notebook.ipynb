{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86f62b0a",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "En un problema de regression (curver fitting), tratamos de encontrar un funcion $f$ que mapear inputs $\\textbf{x}\\in \\mathbb{R}^n$  a sus correspondientes valores de salida $f(\\textbf{x})$: \n",
    "\n",
    "$$f: \\mathbb{R}^n \\rightarrow \\mathbb{R} $$\n",
    "\n",
    "**Notación**: \n",
    "- $\\textbf{X}\\in \\mathbb{R}^{m\\times D}$: matriz de inputs de entrenamiento (training inputs) donde $m$ es el número de ejemplos y $d$ es la dimensión de cada entrada (consideramos un total de D-features en los datos).  \n",
    "- $\\textbf{y}_n\\in \\mathbb{R}^{D}$: Observación correspondiente al input $x_n$ modelada como: \n",
    "\n",
    "$$y_n = f(x_n) + \\epsilon$$\n",
    "\n",
    "donde $\\epsilon$ reprecenta *ruido aleatorio*, usualmente modelado como una variable aleatoria con media cero. En este caso asumiremos que el ruido aleatorio posee distribución gaussiana, esto es: $\\epsilon \\sim N(0, \\sigma^2)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f84ec4",
   "metadata": {},
   "source": [
    "## Problem Formulation\n",
    "\n",
    "Dado que hemos asumido que el ruido presente en las observaciones es gaussiano, abordamos el problema desde una perspectiva **probabilística**. En particular, modelamos explícitamente la incertidumbre en las observaciones usando una función de **verosimilitud (likelihood)**.\n",
    "\n",
    "Asumimos que para cada entrada $\\textbf{x} \\in \\mathbb{R}^D$, la correspondiente salida $y \\in \\mathbb{R}$ se genera de acuerdo con una distribución normal centrada en $f(\\textbf{x})$, con varianza $\\sigma^2$:\n",
    "\n",
    "$$\n",
    "p(y|\\textbf{x}) = \\mathcal{N}(y|f(\\textbf{x}), \\sigma^2)\n",
    "$$\n",
    "\n",
    "donde:\n",
    "- $p(y|\\textbf{x})$ representa la probabilidad de observar un valor $y$ dado un input $\\textbf{x}$.\n",
    "- $f(\\textbf{x})$ es una función determinística desconocida que deseamos aproximar.\n",
    "\n",
    "El **objetivo** es entonces encontrar una función que aproxime bien $f$ a partir de los datos observados, de forma que generalice a nuevos ejemplos no vistos.\n",
    "\n",
    "En el caso particular de **regresión lineal**, asumimos que la función $f$ pertenece a la familia de funciones lineales en los parámetros $\\bm{\\theta} \\in \\mathbb{R}^D$. Específicamente, el modelo se escribe como:\n",
    "\n",
    "$$\n",
    "y = \\textbf{x}^T \\bm{\\theta} + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\n",
    "$$\n",
    "\n",
    "lo cual implica que:\n",
    "\n",
    "$$\n",
    "p(y|\\textbf{x}, \\bm{\\theta}) = \\mathcal{N}(y|\\textbf{x}^T \\bm{\\theta}, \\sigma^2)\n",
    "$$\n",
    "\n",
    "Aquí, $\\bm{\\theta}$ representa los parámetros del modelo que queremos estimar. La hipótesis subyacente es que los datos siguen una relación lineal con las variables de entrada, y que cualquier desviación se debe al ruido gaussiano."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f8a5b2",
   "metadata": {},
   "source": [
    "## Parameter Estimation\n",
    "\n",
    "Dado un conjunto de entrenamiento $D \\coloneqq \\{(\\textbf{x}_1, y_1), \\dots, (\\textbf{x}_N, y_N)\\}$, donde cada $\\textbf{x}_n \\in \\mathbb{R}^D$ es un vector de entrada y $y_n \\in \\mathbb{R}$ su correspondiente salida observada, queremos estimar los parámetros $\\bm{\\theta}$ que mejor se ajusten a los datos.\n",
    "\n",
    "Asumiendo que los pares $(\\textbf{x}_n, y_n)$ son independientes entre sí, la **función de verosimilitud conjunta** del conjunto de datos se puede factorizar como el producto de las verosimilitudes individuales:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(\\textbf{y}|\\textbf{X}, \\bm{\\theta}) &= \\prod_{n=1}^N p(y_n|\\textbf{x}_n, \\bm{\\theta}) \\\\\n",
    "                                     &= \\prod_{n=1}^N \\mathcal{N}(y_n|\\textbf{x}_n^T \\bm{\\theta}, \\sigma^2)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "donde:\n",
    "- $\\textbf{X} \\in \\mathbb{R}^{N \\times D}$ es la matriz de diseño (cada fila es un vector $\\textbf{x}_n^T$),\n",
    "- $\\textbf{y} \\in \\mathbb{R}^N$ es el vector columna de salidas observadas.\n",
    "\n",
    "Esta formulación nos permite emplear técnicas de estimación, como la **estimación por máxima verosimilitud (MLE)**, para encontrar los parámetros óptimos $\\bm{\\theta}$ que maximizan la probabilidad de los datos observados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77fea6b",
   "metadata": {},
   "source": [
    "### Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "La estimación por máxima verosimilitud (MLE) consiste en encontrar los parámetros $\\bm{\\theta}$ que maximizan la verosimilitud de los datos observados. Dado que asumimos que las observaciones $y_n$ son generadas según una distribución gaussiana centrada en $\\bm{x}_n^T\\bm{\\theta}$ con varianza $\\sigma^2$, la función de verosimilitud para el conjunto de entrenamiento es:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\bm{\\theta}) = p(\\bm{y}|\\bm{X}, \\bm{\\theta}) = \\prod_{n=1}^{N} \\mathcal{N}(y_n|\\bm{x}_n^T\\bm{\\theta}, \\sigma^2)\n",
    "$$\n",
    "\n",
    "Para facilitar la optimización, trabajamos con el logaritmo de la verosimilitud (log-likelihood):\n",
    "\n",
    "$$\n",
    "\\log \\mathcal{L}(\\bm{\\theta}) = \\sum_{n=1}^{N} \\log \\left[ \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(y_n - \\bm{x}_n^T\\bm{\\theta})^2}{2\\sigma^2} \\right) \\right]\n",
    "$$\n",
    "\n",
    "Simplificando:\n",
    "\n",
    "$$\n",
    "\\log \\mathcal{L}(\\bm{\\theta}) = -\\frac{N}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{n=1}^{N} (y_n - \\bm{x}_n^T\\bm{\\theta})^2\n",
    "$$\n",
    "\n",
    "Como el primer término no depende de $\\bm{\\theta}$, maximizar la log-verosimilitud equivale a minimizar la siguiente función:\n",
    "\n",
    "$$\n",
    "\\sum_{n=1}^{N} (y_n - \\bm{x}_n^T\\bm{\\theta})^2\n",
    "$$\n",
    "\n",
    "Esta expresión corresponde a la **función de pérdida cuadrática** o **Mean Squared Error (MSE)** (sin la constante de normalización):\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\bm{\\theta}) = \\|\\bm{y} - \\bm{X}\\bm{\\theta} \\|^2\n",
    "$$\n",
    "\n",
    "Por tanto, el objetivo de la regresión lineal es encontrar los parámetros $\\bm{\\theta}$ que minimicen la suma de los errores cuadrados entre las predicciones $\\bm{X}\\bm{\\theta}$ y los valores reales $\\bm{y}$.\n",
    "\n",
    "Para encontrar el mínimo, derivamos con respecto a $\\bm{\\theta}$ e igualamos a cero:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\bm{\\theta}} \\|\\bm{y} - \\bm{X}\\bm{\\theta} \\|^2 = -2\\bm{X}^T(\\bm{y} - \\bm{X}\\bm{\\theta}) = 0\n",
    "$$\n",
    "\n",
    "Resolviendo esta ecuación obtenemos la **solución cerrada (closed-form)** de la regresión lineal:\n",
    "\n",
    "$$\n",
    "\\bm{\\theta}_{\\text{MLE}} = (\\bm{X}^T\\bm{X})^{-1} \\bm{X}^T \\bm{y}\n",
    "$$\n",
    "\n",
    "Esta expresión se conoce como la **solución de los mínimos cuadrados ordinarios** (OLS: *Ordinary Least Squares*).\n",
    "\n",
    "> **Nota:** Esta solución requiere que $\\bm{X}^T\\bm{X}$ sea invertible. Si no lo es (por ejemplo, si hay colinealidad entre columnas de $\\bm{X}$), se deben usar métodos alternativos como la **regresión ridge** o **pseudoinversas**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffd9ad6",
   "metadata": {},
   "source": [
    "### Overfitting in Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaf7beb",
   "metadata": {},
   "source": [
    "### Maximum a Posteriori Estimation (MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f04a5d",
   "metadata": {},
   "source": [
    "### MAP as Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f447c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf52e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression: \n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlvirt",
   "language": "python",
   "name": "mlvirt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
